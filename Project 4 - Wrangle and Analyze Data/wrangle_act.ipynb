{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "twt_archive = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL =\" https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "ImagePrediction = pd.read_csv('image-predictions.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Downloading the file by Requests library via URL provided \n",
    "url = \"https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'access_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-957472a6d2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mccess_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'HIDDEN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_on_rate_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'access_token' is not defined"
     ]
    }
   ],
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "ccess_token = 'HIDDEN'\n",
    "ccess_secret = 'HIDDEN'\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "#tweet_ids = df_1.tweet_id.values\n",
    "#len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "#count = 0\n",
    "#fails_dict = {}\n",
    "#start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "#with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    #for tweet_id in tweet_ids:\n",
    "     #   count += 1\n",
    "     #   print(str(count) + \": \" + str(tweet_id))\n",
    "      #  try:\n",
    "      #      tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "      #      print(\"Success\")\n",
    "      #      json.dump(tweet._json, outfile)\n",
    "      #      outfile.write('\\n')\n",
    "      #  except tweepy.TweepError as e:\n",
    "       #     print(\"Fail\")\n",
    "       #     fails_dict[tweet_id] = e\n",
    "       #     pass\n",
    "#end = timer()\n",
    "#print(end - start)\n",
    "#print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will be using Udacity tweet json file \n",
    "myList = []\n",
    "\n",
    "with open('tweet-json.txt') as file:\n",
    "    lin = file.readlines()\n",
    "    for waad in lin :\n",
    "        load_waad = json.loads(waad)\n",
    "        \n",
    "        tweet_id =load_waad['id']\n",
    "        \n",
    "        retweet_counting =load_waad['retweet_count']\n",
    "        \n",
    "        favorite_counting =load_waad['favorite_count']\n",
    "        \n",
    "        myList.append({'tweet_id': tweet_id ,\n",
    "                        'retweet_counting': retweet_counting,\n",
    "                        'favorite_counting': favorite_counting})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the file \n",
    "twtAPI = pd.DataFrame(myList, columns = ['tweet_id', 'retweet_counting', 'favorite_counting'])\n",
    "twtAPI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Twitter Archive dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will try to find the rating_denominator that equals 0\n",
    "twt_archive[twt_archive['rating_denominator'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Image Prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ImagePrediction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePrediction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePrediction.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePrediction.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Twitter API dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twtAPI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twtAPI.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twtAPI.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Summary:\n",
    "\n",
    "**1.Twitter Archive dataframe**\n",
    "\n",
    "`Quality issues :`\n",
    "\n",
    "1. There is two wrong data type for both (timestamp& retweeted_status_timestamp) should be converted to \"datetime\" insted of \"Object\".\n",
    "\n",
    "2. Many Mising values in multiple columns sush as (in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id, retweeted_status_timestamp,expanded_urls).\n",
    "\n",
    "3. A wrong data type for(tweet_id) should be converted to \"object\" insted of \"Intger\".\n",
    "\n",
    "4. The \"rating denominator\" should be always/almost out of 10, but there are numerous other strange numbers, like ( 110,50,170....etc).\n",
    "\n",
    "5. There are numerous NaN values in the expanded urls column.\n",
    "\n",
    "6. Some of the tweet does not iclude image.\n",
    "\n",
    "7. The preducations name need to be captlized.\n",
    "\n",
    "8. long sources information replace it with someting more readable.\n",
    "\n",
    "9. There is some tweet apears to be a juste retweet/replies.\n",
    "\n",
    "`Tidiness issues : `\n",
    "- \"Dog stages > doggo,floofer,pupper,puppo\" were discovered to be in multiple column so they should be combined into one column.\n",
    "\n",
    "**2.Image Prediction dataframe**\n",
    "\n",
    "`Quality issues :`\n",
    "- There is missing images since it's only 2057 image out of 2356.\n",
    "- wrong datatype (tweet_id).\n",
    "\n",
    "`Tidiness issues : `\n",
    "- Shall be combined with the twitter archive data frame.\n",
    "\n",
    "**3.Twitter API dataframe**\n",
    "\n",
    "`Quality issues :`\n",
    "- wrong datatype (tweet_id).\n",
    "- Missing tweets.\n",
    "\n",
    "`Tidiness issues : `\n",
    "- Shall be combined with the twitter archive data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "In this section, clean **all** of the issues you documented while assessing. \n",
    "\n",
    "**Note:** Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original pieces of data\n",
    "twt_archive_clean = twt_archive.copy()\n",
    "ImagePrediction_clean = ImagePrediction.copy()\n",
    "twtAPI_clean = twtAPI.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twt_archive_clean.shape)\n",
    "print(ImagePrediction_clean.shape)\n",
    "print(twtAPI_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness issues : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Multiple dog stage column \n",
    "**Define:** \"Dog stages > doggo,floofer,pupper,puppo\" were discovered to be in multiple column so they will be combined into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt_archive_clean.loc[(twt_archive_clean[['doggo', 'floofer', 'pupper', 'puppo']] != 'None'\n",
    "                 ).sum(axis=1) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# will be merge different dog types into a one column\n",
    "twt_archive_clean['dog_stages'] = twt_archive_clean[['puppo', 'pupper', 'floofer', 'doggo']].apply(\n",
    "    lambda x: ','.join(x.astype(str)),axis=1)\n",
    "\n",
    "twt_archive_clean['dog_stages'] = twt_archive_clean['dog_stages'].str.replace(r'(,None)', repl='')\n",
    "\n",
    "twt_archive_clean['dog_stages'] = twt_archive_clean['dog_stages'].str.replace(r'(None,)', repl='')\n",
    "\n",
    "twt_archive_clean.drop(['puppo','pupper','floofer','doggo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.dog_stages.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Multiple dataframe \n",
    "**Define:** will be combining all the 3 dataframe into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt_archive_clean = pd.merge(twt_archive_clean,  twtAPI_clean,\n",
    "                            on = ['tweet_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean = pd.merge(twt_archive_clean, ImagePrediction_clean,\n",
    "                            on = ['tweet_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test \n",
    "twt_archive_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test \n",
    "twt_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "**Define:** will be removing all replies and retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean = twt_archive_clean[pd.isnull(twt_archive_clean.retweeted_status_id)]\n",
    "twt_archive_clean= twt_archive_clean[pd.isnull(twt_archive_clean.in_reply_to_status_id)]\n",
    "twt_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "**Define:** will be converting the datatype of\"timestamp and retweeted_status_timestamp\" from object to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "twt_archive_clean['timestamp']= pd.to_datetime(twt_archive_clean['timestamp'])\n",
    "twt_archive_clean['retweeted_status_timestamp']= pd.to_datetime(twt_archive_clean['retweeted_status_timestamp'])\n",
    "#Test\n",
    "twt_archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "twt_archive_clean.timestamp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "**Define:** Many Mising values in multiple columns sush as (in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id, retweeted_status_timestamp,expanded_urls) will be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean = twt_archive_clean.drop(['in_reply_to_user_id','retweeted_status_user_id'\n",
    "                                            ,'retweeted_status_timestamp','retweeted_status_id','in_reply_to_status_id'] ,1)\n",
    "\n",
    "twt_archive_clean.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "### 4.\n",
    "**Define:** will be converting the datatype of\"tweet_id\" from intger to object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean['tweet_id'] = twt_archive_clean['tweet_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. \n",
    "**Define:** The \"rating denominator\" should be always/almost out of 10, but there are numerous other strange numbers, like ( 110,50,170....etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = twt_archive_clean[twt_archive_clean['rating_numerator'] <= 10]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "**Define:** reomving long sources information replace it with someting more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the sources\n",
    "twt_archive_clean.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.source = twt_archive_clean.source.replace({'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>':'Twitter for iPhone',\n",
    "                                                                     '<a href=\"http://vine.co\" rel=\"nofollow\">Vine - Make a Scene</a>':'Vine - Make a Scene',\n",
    "                                                                     '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>': 'Twitter Web Client',\n",
    "                                                                     '<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>': 'TweetDeck'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test \n",
    "twt_archive_clean.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. \n",
    "**Define:** There are numerous NaN values in the expanded urls column will be removed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean[twt_archive_clean['expanded_urls'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.dropna(subset =['expanded_urls'],inplace=True)\n",
    "twt_archive_clean.expanded_urls.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "**Define:** some tweet does not have image ,will be deleting the tweet that does not iclude images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean = twt_archive_clean[twt_archive_clean.jpg_url.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "twt_archive_clean.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. \n",
    "**Define:** captlizing the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.p1 =twt_archive_clean.p1.str.title()\n",
    "twt_archive_clean.p2 =twt_archive_clean.p2.str.title()\n",
    "twt_archive_clean.p3 =twt_archive_clean.p3.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "twt_archive_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt_archive_clean.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "In this section, analyze and visualize your wrangled data. You must produce at least **three (3) insights and one (1) visualization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b=twt_archive_clean.source.value_counts()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v1 = b.plot.bar(color = 'green', fontsize = 10)\n",
    "\n",
    "\n",
    "plt.title('Common Twitter sources', color = 'black', fontsize = '15')\n",
    "plt.xlabel('Source', color = 'black', fontsize = '10')\n",
    "plt.ylabel('counts of tweets', color = 'black', fontsize = '10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=twt_archive_clean.dog_stages.value_counts()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v2 = a.plot.bar(color = 'blue', fontsize = 10)\n",
    "\n",
    "\n",
    "plt.title('Common dog stages', color = 'black', fontsize = '15')\n",
    "plt.xlabel('Dog Stage', color = 'black', fontsize = '10')\n",
    "plt.ylabel('Counts', color = 'black', fontsize = '10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = a\n",
    "labels = ['None', 'pupper', 'doggo', 'puppo', 'pupper,doggo','floofer','floofer,doggo','puppo,doggo' ]\n",
    "plt.pie(arr, labels = labels , radius = 1.40)\n",
    "plt.legend(title = \"Dog Stages\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "1.The first chart shows that the most commn used source by pepole is through the phones with 1932.\n",
    "\n",
    "2.The first chart shows the least way of using twitter is through deck with 11.\n",
    "\n",
    "3.The first chart shows that there are significant gaps between commonly used which makes it clear that some people would prefer to use Twitter on their phones.\n",
    "\n",
    "4.The second pie chart shows the higest dog stage were pupper with 201.\n",
    "\n",
    "5.The second pie chart shows the loewst dog stage were floofer with 7.\n",
    "\n",
    "6.From the pie chart we can see that some tweets do not include dog stage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
